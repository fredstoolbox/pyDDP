# pyDDP
Use PyTorch's distributed data parallel to train a simple model in a multi-GPU environment. The example includes demonstration of using gradient accumulation in training. Hyper params are included in the __main__ function of the script, use `python ./Simple_DDP.py` to start it
